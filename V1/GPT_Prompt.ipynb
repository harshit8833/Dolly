{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_D5YiDSfWHG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    if data_point[\"instruction\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"instruction\": \"Write a summary of the given article.\",\n",
        "        \"input\": \"Article: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
        "        \"output\": \"A summary of the article should capture the main points and key information.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Translate the following sentence to French.\",\n",
        "        \"input\": \"Sentence: Hello, how are you?\",\n",
        "        \"output\": \"Translation: Bonjour, comment Ã§a va ?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "tokenized_data = []\n",
        "\n",
        "for data_point in data:\n",
        "    prompt = generate_prompt(data_point)\n",
        "    tokenized_prompt = tokenizer.encode_plus(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_data.append({\"prompt\": tokenized_prompt[\"input_ids\"], \"attention_mask\": tokenized_prompt[\"attention_mask\"]})\n",
        "\n",
        "print(tokenized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Prompt---->\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write a summary of the given article.\n",
        "\n",
        "### Input:\n",
        "Article: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam ac gravida nulla. Integer non velit sed nisl fringilla consequat. Donec feugiat metus vitae velit ullamcorper, ut interdum ligula gravida.\n",
        "\n",
        "### Response:\n",
        "A summary of the article should capture the main points and key information.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Vwu_bTXfkuBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "tokenized_data---->\n",
        "[\n",
        "    {\n",
        "        'prompt': tensor([[   61,  3350,    61,  1537,  2570,   338,  2853,  2794,   462,  1467,\n",
        "            147,  4597,   257,  2848,  1107,  1210,  1537,  2570,    13,     1,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "              0,     0,     0,     0\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "yROK3B4ofefC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}